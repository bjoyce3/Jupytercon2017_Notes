{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [[ -d project-tycho-utilities ]];\n",
    "then\n",
    "  cd project-tycho-utilities/\n",
    "  git pull\n",
    "else\n",
    "  git clone https://github.com/lgautier/project-tycho-utilities.git\n",
    "  cd project-tycho-utilities/\n",
    "fi\n",
    "DBNAME=../tycho.db make all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- label:spark_setup -->\n",
    "Spark can be started from regular Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.conf.SparkConf()\n",
    "(conf.setMaster('local[2]')\n",
    " .setAppName('ipython-notebook')\n",
    " .set(\"spark.executor.memory\", \"2g\"))\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- label:spark_sql_create -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "dbfilename = \"tycho.db\"\n",
    "dbcon = sqlite3.connect(dbfilename)\n",
    "cursor = dbcon.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- label:spark_dataframe -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "sqlcontext = SQLContext(sc)\n",
    "cursor.execute(\"SELECT * FROM location\")\n",
    "location = \\\n",
    "    sqlcontext.createDataFrame(cursor,\n",
    "                               tuple(x[0] for x in cursor.description))\n",
    "location.registerTempTable(\"location\")\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT * \n",
    "FROM (SELECT * FROM disease WHERE name='PNEUMONIA') AS disease\n",
    "INNER JOIN casecount\n",
    "ON disease.id=casecount.disease_id\"\"\"\n",
    "\n",
    "cursor.execute(sql)\n",
    "casecount = \\\n",
    "    sqlcontext.createDataFrame(cursor,\n",
    "                               tuple(x[0] for x in cursor.description))\n",
    "casecount.registerTempTable(\"casecount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming data out of SQLite onto Spark cluster\n",
    "SQL query on database in Spark\n",
    "Regular old SQL\n",
    "    Translated by Spark\n",
    "    optimized\n",
    "    sent to JVM for bytecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "<!-- label:spark_query -->\n",
    "SQL can be used to query the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT state, count(city) AS ct\n",
    "FROM location\n",
    "GROUP BY state\n",
    "ORDER BY ct DESC\n",
    "\"\"\"\n",
    "counts = sqlcontext.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- label:spark_query_collect -->\n",
    "The evaluation is only performed when the results are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = counts.collect()\n",
    "res[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- label:spark_mapreduce -->\n",
    "\n",
    "Spark is particularly comfortable with map-reduce tasks.\n",
    "The input data can be our table (stored in a RDBM).\n",
    "Here we count the number of times suffixes are found in city names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = (location\n",
    "         .filter(location.city.isNotNull())\n",
    "         .rdd\n",
    "         .flatMap(lambda rec: [x[-5:] for x in rec.city.split()])\n",
    "         .map(lambda word: (word.lower(), 1))\n",
    "         .reduceByKey(lambda a, b: a+b))\n",
    "names.takeOrdered(10, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<!-- label:spark_sqlmapreduce -->\n",
    "\n",
    "We can also seamlessly use result table obtained from an SQL query\n",
    "to perform map/reduce tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT city\n",
    "FROM (SELECT * FROM casecount WHERE epiweek LIKE '1912%') AS sub\n",
    "INNER JOIN location\n",
    "ON location.id=sub.location_id\n",
    "GROUP BY city\n",
    "\"\"\"\n",
    "y_1912 = sqlcontext.sql(sql)\n",
    "names = (y_1912\n",
    "         .filter(y_1912.city.isNotNull())\n",
    "         .rdd\n",
    "         .flatMap(lambda rec: [x[-5:] for x in rec.city.split()])\n",
    "         .map(lambda word: (word.lower(), 1))\n",
    "         .reduceByKey(lambda a,b: a+b))\n",
    "names.takeOrdered(5, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<!-- label:spark_sqlmapreduceggplot_1_2 -->\n",
    "\n",
    "# SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## --- SQL ---\n",
    "sql = \"\"\"\n",
    "SELECT state, city, date_from, event, count AS ct\n",
    "FROM (SELECT * FROM casecount WHERE epiweek LIKE '1912%') AS sub\n",
    "INNER JOIN location\n",
    "ON location.id=sub.location_id\n",
    "\"\"\"\n",
    "\n",
    "y_1912 = sqlcontext.sql(sql)\n",
    "\n",
    "## --- Spark ---\n",
    "cases = (y_1912\n",
    "         .rdd\n",
    "         .map(lambda rec: ((rec.state,\n",
    "\t                    int(rec.date_from.split('-')[1]),\n",
    "\t\t\t    rec.event),\n",
    "                           rec.ct))\n",
    "         .reduceByKey(lambda a, b: a + b)).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
